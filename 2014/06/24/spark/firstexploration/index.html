<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="iJZtHJClNVGIGAFwlaAxWI9FszjJPbNgm8uUIfV7tqU" />




  <meta name="baidu-site-verification" content="uPUiNfOySI" />







  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Spark," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="本文主要用于分享Spark的一些感悟">
<meta name="keywords" content="Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark初探——关于分享Spark的一些感悟">
<meta property="og:url" content="https://crazylook.github.io/2014/06/24/spark/firstexploration/index.html">
<meta property="og:site_name" content="crazylook&#39;s blog">
<meta property="og:description" content="本文主要用于分享Spark的一些感悟">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/01.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/09.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/06.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/07.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/08.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/02.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/03.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/04.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/05.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/10.png">
<meta property="og:updated_time" content="2018-02-09T03:55:37.193Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark初探——关于分享Spark的一些感悟">
<meta name="twitter:description" content="本文主要用于分享Spark的一些感悟">
<meta name="twitter:image" content="https://crazylook.github.io/images/2014-6-24/01.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>

  <title> Spark初探——关于分享Spark的一些感悟 | crazylook's blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  



  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?f07d67c93535fcf548e2bc032f8737b9";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>








  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">crazylook's blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Python、Spark、ML、DL、推荐</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Spark初探——关于分享Spark的一些感悟
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2014-06-24T12:20:31+08:00" content="2014-06-24">
              2014-06-24
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文主要用于分享Spark的一些感悟<br><a id="more"></a></p>
<blockquote>
<p><strong>关于作者</strong></p>
<ul>
<li>crazylook: pyhton,spark,ml,dl,recsys</li>
<li>blog: <a href="https://crazylook.github.io/">https://crazylook.github.io/</a></li>
<li>email: liuhongbin1990@foxmail.com</li>
</ul>
<p><strong>转载请注明出处：</strong><br><a href="https://crazylook.github.io/2014/06/24/spark-notes-firstexploration/">https://crazylook.github.io/2014/06/24/spark-notes-firstexploration/</a></p>
</blockquote>
<p><strong>前言</strong><br>之前偶尔遇到Spark、Shark这些名词，因为一直对Hadoop研究还有点粗浅，不愿意去接这触个号称更高一级别的Spark。这两天接触了Spark亚太研究院的一个免费公开课，开始揭开Spark的神秘面纱。<br>下面正式开始干货。</p>
<h1 id="1-Spark为什么能够把Hadoop速度提高100倍以上"><a href="#1-Spark为什么能够把Hadoop速度提高100倍以上" class="headerlink" title="1. Spark为什么能够把Hadoop速度提高100倍以上"></a>1. Spark为什么能够把Hadoop速度提高100倍以上</h1><h2 id="1-1-How-fast-to-write-with-scala"><a href="#1-1-How-fast-to-write-with-scala" class="headerlink" title="1.1 How fast to write with scala?"></a>1.1 How fast to write with scala?</h2><h3 id="1-1-1-Spark的WordCount实例"><a href="#1-1-1-Spark的WordCount实例" class="headerlink" title="1.1.1 Spark的WordCount实例"></a>1.1.1 Spark的WordCount实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val file = sc.textFile(&quot;hdfs://...&quot;)</span><br><span class="line">val counts = file.flatMap(line =&gt; line.split(&quot; &quot;)).map(word =&gt; (word, 1)).reduceByKey(_ + _)</span><br><span class="line">counts.saveAsTextFile(“hdfs://…&quot;)</span><br></pre></td></tr></table></figure>
<p>顿时感觉scala编程很有R的味道（简洁、函数式编程）。本人很喜欢，做为本科数学系的人来说就是喜欢函数式编程。</p>
<h3 id="1-1-2-对比一下Wordcount-in-Hadoop"><a href="#1-1-2-对比一下Wordcount-in-Hadoop" class="headerlink" title="1.1.2 对比一下Wordcount in Hadoop"></a>1.1.2 对比一下Wordcount in Hadoop</h3><img src="/images/2014-6-24/01.png" width="400" height="400">
<p>我想我再也不愿意用Java写mapreduce程序了。</p>
<h3 id="1-1-3-对比一下RHadoop写的Wordcount"><a href="#1-1-3-对比一下RHadoop写的Wordcount" class="headerlink" title="1.1.3 对比一下RHadoop写的Wordcount"></a>1.1.3 对比一下RHadoop写的Wordcount</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input&lt;- &apos;/user/hadoop/input&apos;</span><br><span class="line">wordcount.mr&lt;-mapreduce(</span><br><span class="line">    input,</span><br><span class="line">    map=function(k,v)&#123;</span><br><span class="line">        key&lt;-unlist(strsplit(v,&quot; &quot;))</span><br><span class="line">　　    keyval(key,1)</span><br><span class="line">    &#125;</span><br><span class="line">　　,reduce=function(k,v)&#123;</span><br><span class="line">        keyval(k,sum(v))</span><br><span class="line">　　&#125;</span><br><span class="line">)</span><br><span class="line">from.dfs(wordcount.mr)</span><br></pre></td></tr></table></figure>
<p>其实RHadoop编程也很简洁，但是RHadoop还不是一个可用于生产环境的产品。</p>
<h2 id="1-2MapReduce架构"><a href="#1-2MapReduce架构" class="headerlink" title="1.2MapReduce架构"></a>1.2MapReduce架构</h2><img src="/images/2014-6-24/09.png" width="400" height="400">
<h2 id="1-3Why-Hadoop-so-slow"><a href="#1-3Why-Hadoop-so-slow" class="headerlink" title="1.3Why Hadoop so slow?"></a>1.3Why Hadoop so slow?</h2><img src="/images/2014-6-24/06.png" width="400" height="400">
<h2 id="1-4Spark-So-fast"><a href="#1-4Spark-So-fast" class="headerlink" title="1.4Spark? So fast!"></a>1.4Spark? So fast!</h2><img src="/images/2014-6-24/07.png" width="400" height="400">
<p><strong>More reasons!</strong></p>
<ul>
<li>DAG     有向无环图</li>
<li><p>Scheduler   调度</p>
<img src="/images/2014-6-24/08.png" width="400" height="400">
</li>
<li><p>Lineage     血统(容错处理)</p>
</li>
</ul>
<h1 id="2-Spark的内核剖析"><a href="#2-Spark的内核剖析" class="headerlink" title="2. Spark的内核剖析"></a>2. Spark的内核剖析</h1><h2 id="2-1-一些令人激动的图"><a href="#2-1-一些令人激动的图" class="headerlink" title="2.1 一些令人激动的图"></a>2.1 一些令人激动的图</h2><h3 id="2-1-1-上一个高大上的架构图"><a href="#2-1-1-上一个高大上的架构图" class="headerlink" title="2.1.1 上一个高大上的架构图"></a>2.1.1 上一个高大上的架构图</h3><img src="/images/2014-6-24/02.png" width="400" height="400">
<h3 id="2-1-2-One-stack-to-rule-them-all"><a href="#2-1-2-One-stack-to-rule-them-all" class="headerlink" title="2.1.2 One stack to rule them all"></a>2.1.2 One stack to rule them all</h3><p>用Spark，一个团队可以干Hadoop三个不同团队干的活。<br><img src="/images/2014-6-24/03.png" width="400" height="400"></p>
<h3 id="2-1-3-Project-Goals"><a href="#2-1-3-Project-Goals" class="headerlink" title="2.1.3 Project  Goals"></a>2.1.3 Project  Goals</h3><img src="/images/2014-6-24/04.png" width="400" height="400">
<h3 id="2-1-4-Codebase-size"><a href="#2-1-4-Codebase-size" class="headerlink" title="2.1.4 Codebase size"></a>2.1.4 Codebase size</h3><p>这个是一个让我发狂的东东，Spark主要内核代码只有2W行，这点燃了我研究Spark源码的欲望<br><img src="/images/2014-6-24/05.png" width="200" height="200"></p>
<h2 id="2-2-Spark核心概念"><a href="#2-2-Spark核心概念" class="headerlink" title="2.2 Spark核心概念"></a>2.2 Spark核心概念</h2><h3 id="2-2-1-RDD-弹性分布数据集"><a href="#2-2-1-RDD-弹性分布数据集" class="headerlink" title="2.2.1 (RDD)弹性分布数据集"></a>2.2.1 (RDD)弹性分布数据集</h3><ul>
<li>RDD(Resilient Distributed Dataset)是Spark的最基本抽象,是对分布式内存的抽象使用，实现了以操作本地集合的方式来操作分布式数据集的抽象实现。RDD是Spark最核心的东西，它表示已被分区，不可变的并能够被并行操作的数据集合，不同的数据集格式对应不同的RDD实现。RDD必须是可序列化的。RDD可以cache到内存中，每次对RDD数据集的操作之后的结果，都可以存放到内存中，下一个操作可以直接从内存中输入，省去了MapReduce大量的磁盘IO操作。这对于迭代运算比较常见的机器学习算法, 交互式数据挖掘来说，效率提升比较大。</li>
<li>RDD的特点：</li>
</ul>
<ol>
<li>它是在集群节点上的不可变的、已分区的集合对象。</li>
<li>通过并行转换的方式来创建如（map, filter, join, etc）。</li>
<li>失败自动重建。</li>
<li>可以控制存储级别（内存、磁盘等）来进行重用。</li>
<li>必须是可序列化的。</li>
<li>是静态类型的。</li>
</ol>
<ul>
<li>RDD的好处</li>
</ul>
<ol>
<li>RDD只能从持久存储或通过Transformations操作产生，相比于分布式共享内存（DSM）可以更高效实现容错，对于丢失部分数据分区只需根据它的lineage就可重新计算出来，而不需要做特定的Checkpoint。</li>
<li>RDD的不变性，可以实现类Hadoop MapReduce的推测式执行。</li>
<li>RDD的数据分区特性，可以通过数据的本地性来提高性能，这与Hadoop MapReduce是一样的。</li>
<li>RDD都是可序列化的，在内存不足时可自动降级为磁盘存储，把RDD存储于磁盘上，这时性能会有大的下降但不会差于现在的MapReduce。</li>
</ol>
<ul>
<li>RDD的存储与分区</li>
</ul>
<ol>
<li>用户可以选择不同的存储级别存储RDD以便重用。</li>
<li>当前RDD默认是存储于内存，但当内存不足时，RDD会spill到disk。</li>
<li>RDD在需要进行分区把数据分布于集群中时会根据每条记录Key进行分区（如Hash 分区），以此保证两个数据集在Join时能高效。</li>
</ol>
<ul>
<li>RDD的内部表示<br>在RDD的内部实现中每个RDD都可以使用5个方面的特性来表示：</li>
</ul>
<ol>
<li>分区列表（数据块列表）</li>
<li>计算每个分片的函数（根据父RDD计算出此RDD）</li>
<li>对父RDD的依赖列表</li>
<li>对key-value RDD的Partitioner【可选】</li>
<li>每个数据分片的预定义地址列表(如HDFS上的数据块的地址)【可选】</li>
</ol>
<ul>
<li><p>RDD的存储级别<br>RDD根据useDisk、useMemory、deserialized、replication四个参数的组合提供了11种存储级别：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val DISK_ONLY = new StorageLevel(true, false, false)</span><br><span class="line">val DISK_ONLY_2 = new StorageLevel(true, false, false, 2)</span><br><span class="line">val MEMORY_ONLY = new StorageLevel(false, true, true)</span><br><span class="line">val MEMORY_ONLY_2 = new StorageLevel(false, true, true, 2)</span><br><span class="line">val MEMORY_ONLY_SER = new StorageLevel(false, true, false)</span><br><span class="line">val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, 2)</span><br><span class="line">val MEMORY_AND_DISK = new StorageLevel(true, true, true)</span><br><span class="line">val MEMORY_AND_DISK_2 = new StorageLevel(true, true, true, 2)</span><br><span class="line">val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false)</span><br><span class="line">val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, 2)</span><br></pre></td></tr></table></figure>
</li>
<li><p>RDD定义了各种操作，不同类型的数据由不同的RDD类抽象表示，不同的操作也由RDD进行抽实现。</p>
</li>
</ul>
<h3 id="2-2-2-Lineage（血统）"><a href="#2-2-2-Lineage（血统）" class="headerlink" title="2.2.2 Lineage（血统）"></a>2.2.2 Lineage（血统）</h3><ul>
<li>利用内存加快数据加载,在众多的其它的In-Memory类数据库或Cache类系统中也有实现，Spark的主要区别在于它处理分布式运算环境下的数据容错性（节点实效/数据丢失）问题时采用的方案。为了保证RDD中数据的鲁棒性，RDD数据集通过所谓的血统关系(Lineage)记住了它是如何从其它RDD中演变过来的。相比其它系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的特定数据转换（Transformation）操作（filter, map, join etc.)行为。当这个RDD的部分分区数据丢失时，它可以通过Lineage获取足够的信息来重新运算和恢复丢失的数据分区。这种粗颗粒的数据模型，限制了Spark的运用场合，但同时相比细颗粒度的数据模型，也带来了性能的提升。</li>
<li>RDD在Lineage依赖方面分为两种Narrow Dependencies与Wide Dependencies用来解决数据容错的高效性。Narrow Dependencies是指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区或多个父RDD的分区对应于一个子RDD的分区，也就是说一个父RDD的一个分区不可能对应一个子RDD的多个分区。Wide Dependencies是指子RDD的分区依赖于父RDD的多个分区或所有分区，也就是说存在一个父RDD的一个分区对应一个子RDD的多个分区。对与Wide Dependencies，这种计算的输入和输出在不同的节点上，lineage方法对与输入节点完好，而输出节点宕机时，通过重新计算，这种情况下，这种方法容错是有效的，否则无效，因为无法重试，需要向上其祖先追溯看是否可以重试（这就是lineage，血统的意思），Narrow Dependencies对于数据的重算开销要远小于Wide Dependencies的数据重算开销。</li>
</ul>
<h3 id="2-2-3-容错"><a href="#2-2-3-容错" class="headerlink" title="2.2.3 容错"></a>2.2.3 容错</h3><ul>
<li>在RDD计算，通过checkpint进行容错，做checkpoint有两种方式，一个是checkpoint data，一个是logging the updates。用户可以控制采用哪种方式来实现容错，默认是logging the updates方式，通过记录跟踪所有生成RDD的转换（transformations）也就是记录每个RDD的lineage（血统）来重新计算生成丢失的分区数据。</li>
</ul>
<h3 id="2-2-4-资源管理与作业调度"><a href="#2-2-4-资源管理与作业调度" class="headerlink" title="2.2.4 资源管理与作业调度"></a>2.2.4 资源管理与作业调度</h3><ul>
<li>Spark对于资源管理与作业调度可以使用Standalone(独立模式)，Apache Mesos及Hadoop YARN来实现。 Spark on Yarn在Spark0.6时引用，但真正可用是在现在的branch-0.8版本。Spark on Yarn遵循YARN的官方规范实现，得益于Spark天生支持多种Scheduler和Executor的良好设计，对YARN的支持也就非常容易，Spark on Yarn的大致框架图。<img src="/images/2014-6-24/10.png" width="600" height="600"></li>
<li>让Spark运行于YARN上与Hadoop共用集群资源可以提高资源利用率。</li>
</ul>
<h3 id="2-2-5-Scala"><a href="#2-2-5-Scala" class="headerlink" title="2.2.5 Scala"></a>2.2.5 Scala</h3><ul>
<li>Spark使用Scala开发，默认使用Scala作为编程语言。编写Spark程序比编写Hadoop MapReduce程序要简单的多，SparK提供了Spark-Shell，可以在Spark-Shell测试程序。写SparK程序的一般步骤就是创建或使用(SparkContext)实例，使用SparkContext创建RDD，然后就是对RDD进行操作。如：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val sc = new SparkContext(master, appName, [sparkHome], [jars])</span><br><span class="line">val textFile = sc.textFile(&quot;hdfs://.....&quot;)</span><br><span class="line">textFile.map(....).filter(.....).....</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>Scala是我接触的除了R又一个令我激动的语言，有机会要好好研究一下。</p>
<h1 id="3-Spark集群案例解析，包含集群搭建"><a href="#3-Spark集群案例解析，包含集群搭建" class="headerlink" title="3.Spark集群案例解析，包含集群搭建"></a>3.Spark集群案例解析，包含集群搭建</h1><h2 id="3-1-Standalone模式"><a href="#3-1-Standalone模式" class="headerlink" title="3.1 Standalone模式"></a>3.1 Standalone模式</h2><ul>
<li>为方便Spark的推广使用，Spark提供了Standalone模式，Spark一开始就设计运行于Apache Mesos资源管理框架上，这是非常好的设计，但是却带了部署测试的复杂性。为了让Spark能更方便的部署和尝试，Spark因此提供了Standalone运行模式，它由一个Spark Master和多个Spark worker组成，与Hadoop MapReduce1很相似，就连集群启动方式都几乎是一样。</li>
<li>以Standalone模式运行Spark集群</li>
</ul>
<ol>
<li>下载Scala2.9.3，并配置SCALA_HOME</li>
<li>下载Spark代码（可以使用源码编译也可以下载编译好的版本）这里下载 编译好的版本（<a href="http://spark-project.org/download/spark-0.7.3-prebuilt-cdh4.tgz）" target="_blank" rel="noopener">http://spark-project.org/download/spark-0.7.3-prebuilt-cdh4.tgz）</a></li>
<li>解压spark-0.7.3-prebuilt-cdh4.tgz安装包</li>
<li><p>修改配置（conf/*） slaves: 配置工作节点的主机名 spark-env.sh：配置环境变量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SCALA_HOME=/home/spark/scala-2.9.3</span><br><span class="line">JAVA_HOME=/home/spark/jdk1.6.0_45</span><br><span class="line">SPARK_MASTER_IP=spark1             </span><br><span class="line">SPARK_MASTER_PORT=30111</span><br><span class="line">SPARK_MASTER_WEBUI_PORT=30118</span><br><span class="line">SPARK_WORKER_CORES=2 SPARK_WORKER_MEMORY=4g</span><br><span class="line">SPARK_WORKER_PORT=30333</span><br><span class="line">SPARK_WORKER_WEBUI_PORT=30119</span><br><span class="line">SPARK_WORKER_INSTANCES=1</span><br></pre></td></tr></table></figure>
</li>
<li><p>把Hadoop配置copy到conf目录下</p>
</li>
<li>在master主机上对其它机器做ssh无密码登录</li>
<li>把配置好的Spark程序使用scp copy到其它机器</li>
<li>在master启动集群<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$SPARK_HOME/start-all.sh</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="3-2-yarn模式"><a href="#3-2-yarn模式" class="headerlink" title="3.2 yarn模式"></a>3.2 yarn模式</h2><ul>
<li>Spark-shell现在还不支持Yarn模式，使用Yarn模式运行，需要把Spark程序全部打包成一个jar包提交到Yarn上运行。目录只有branch-0.8版本才真正支持Yarn。</li>
<li>以Yarn模式运行Spark</li>
</ul>
<ol>
<li>下载Spark代码.<br><code>git clone git://github.com/mesos/spark</code></li>
<li><p>切换到branch-0.8</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd spark</span><br><span class="line">git checkout -b yarn --track origin/yarn</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用sbt编译Spark并</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$SPARK_HOME/sbt/sbt</span><br><span class="line">package</span><br><span class="line">assembly</span><br></pre></td></tr></table></figure>
</li>
<li><p>把Hadoop yarn配置copy到conf目录下</p>
</li>
<li>运行测试<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> SPARK_JAR=./core/target/scala-2.9.3/spark-core-assembly-0.8.0-SNAPSHOT.jar \</span><br><span class="line">./run spark.deploy.yarn.Client --jar examples/target/scala-2.9.3/ \</span><br><span class="line">--class spark.examples.SparkPi --args yarn-standalone</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="3-3-使用Spark-shell"><a href="#3-3-使用Spark-shell" class="headerlink" title="3.3 使用Spark-shell"></a>3.3 使用Spark-shell</h2><ul>
<li>Spark-shell使用很简单，当Spark以Standalon模式运行后，使用$SPARK_HOME/spark-shell进入shell即可，在Spark-shell中SparkContext已经创建好了，实例名为sc可以直接使用，还有一个需要注意的是，在Standalone模式下，Spark默认使用的调度器的FIFO调度器而不是公平调度，而Spark-shell作为一个Spark程序一直运行在Spark上，其它的Spark程序就只能排队等待，也就是说同一时间只能有一个Spark-shell在运行。</li>
<li>在Spark-shell上写程序非常简单，就像在Scala Shell上写程序一样。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val textFile = sc.textFile(&quot;hdfs://hadoop1:2323/user/data&quot;)</span><br><span class="line">textFile: spark.RDD[String] = spark.MappedRDD@2ee9b6e3</span><br><span class="line"></span><br><span class="line">scala&gt; textFile.count() // Number of items in this RDD</span><br><span class="line">res0: Long = 21374</span><br><span class="line"></span><br><span class="line">scala&gt; textFile.first() // First item in this RDD</span><br><span class="line">res1: String = # Spark</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="3-4-编写Driver程序"><a href="#3-4-编写Driver程序" class="headerlink" title="3.4 编写Driver程序"></a>3.4 编写Driver程序</h2><ul>
<li>在Spark中Spark程序称为Driver程序，编写Driver程序很简单几乎与在Spark-shell上写程序是一样的，不同的地方就是SparkContext需要自己创建。如WorkCount程序如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line"></span><br><span class="line">object WordCount &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    if (args.length ==0 )&#123;</span><br><span class="line">      println(&quot;usage is org.test.WordCount &lt;master&gt;&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    println(&quot;the args: &quot;)</span><br><span class="line">    args.foreach(println)</span><br><span class="line"></span><br><span class="line">    val hdfsPath = &quot;hdfs://hadoop1:8020&quot;</span><br><span class="line"></span><br><span class="line">    // create the SparkContext， args(0)由yarn传入appMaster地址</span><br><span class="line">    val sc = new SparkContext(args(0), &quot;WrodCount&quot;,</span><br><span class="line">    System.getenv(&quot;SPARK_HOME&quot;), Seq(System.getenv(&quot;SPARK_TEST_JAR&quot;)))</span><br><span class="line"></span><br><span class="line">    val textFile = sc.textFile(hdfsPath + args(1))</span><br><span class="line"></span><br><span class="line">    val result = textFile.flatMap(line =&gt; line.split(&quot;\\s+&quot;))</span><br><span class="line">        .map(word =&gt; (word, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    result.saveAsTextFile(hdfsPath + args(2))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>附：一篇介绍Spark比较到位的Page</strong><br><a href="http://tech.uc.cn/?p=2116" target="_blank" rel="noopener">http://tech.uc.cn/?p=2116</a></p>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Spark/" rel="tag">#Spark</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2014/06/18/demo/mahout-notes-day1/" rel="next" title="Mahout开发环境搭建及推荐系统初探">
                <i class="fa fa-chevron-left"></i> Mahout开发环境搭建及推荐系统初探
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2014/06/30/demo/vb-project-backup/" rel="prev" title="研二这一年那点项目">
                研二这一年那点项目 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="crazylook" />
          <p class="site-author-name" itemprop="name">crazylook</p>
          <p class="site-description motion-element" itemprop="description">Python、Spark、ML、DL、推荐</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">13</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">15</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Spark为什么能够把Hadoop速度提高100倍以上"><span class="nav-number">1.</span> <span class="nav-text">1. Spark为什么能够把Hadoop速度提高100倍以上</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-How-fast-to-write-with-scala"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 How fast to write with scala?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-1-Spark的WordCount实例"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1.1 Spark的WordCount实例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-2-对比一下Wordcount-in-Hadoop"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.1.2 对比一下Wordcount in Hadoop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-3-对比一下RHadoop写的Wordcount"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.1.3 对比一下RHadoop写的Wordcount</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2MapReduce架构"><span class="nav-number">1.2.</span> <span class="nav-text">1.2MapReduce架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3Why-Hadoop-so-slow"><span class="nav-number">1.3.</span> <span class="nav-text">1.3Why Hadoop so slow?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4Spark-So-fast"><span class="nav-number">1.4.</span> <span class="nav-text">1.4Spark? So fast!</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Spark的内核剖析"><span class="nav-number">2.</span> <span class="nav-text">2. Spark的内核剖析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-一些令人激动的图"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 一些令人激动的图</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-上一个高大上的架构图"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.1.1 上一个高大上的架构图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-One-stack-to-rule-them-all"><span class="nav-number">2.1.2.</span> <span class="nav-text">2.1.2 One stack to rule them all</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-3-Project-Goals"><span class="nav-number">2.1.3.</span> <span class="nav-text">2.1.3 Project  Goals</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-4-Codebase-size"><span class="nav-number">2.1.4.</span> <span class="nav-text">2.1.4 Codebase size</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-Spark核心概念"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 Spark核心概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-RDD-弹性分布数据集"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1 (RDD)弹性分布数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-Lineage（血统）"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2 Lineage（血统）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-容错"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.2.3 容错</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-4-资源管理与作业调度"><span class="nav-number">2.2.4.</span> <span class="nav-text">2.2.4 资源管理与作业调度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-5-Scala"><span class="nav-number">2.2.5.</span> <span class="nav-text">2.2.5 Scala</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Spark集群案例解析，包含集群搭建"><span class="nav-number">3.</span> <span class="nav-text">3.Spark集群案例解析，包含集群搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-Standalone模式"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 Standalone模式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-yarn模式"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 yarn模式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-使用Spark-shell"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 使用Spark-shell</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-编写Driver程序"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 编写Driver程序</span></a></li></ol></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2014 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">crazylook</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  



  
  
  

  

  

</body>
</html>
