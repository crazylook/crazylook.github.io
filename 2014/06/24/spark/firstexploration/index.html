<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Spark初探——关于分享Spark的一些感悟 | crazylook&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="本文主要用于分享Spark的一些感悟">
<meta name="keywords" content="Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark初探——关于分享Spark的一些感悟">
<meta property="og:url" content="https://crazylook.github.io/2014/06/24/spark/firstexploration/index.html">
<meta property="og:site_name" content="crazylook&#39;s blog">
<meta property="og:description" content="本文主要用于分享Spark的一些感悟">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/01.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/09.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/06.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/07.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/08.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/02.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/03.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/04.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/05.png">
<meta property="og:image" content="https://crazylook.github.io/images/2014-6-24/10.png">
<meta property="og:updated_time" content="2018-02-09T03:55:37.193Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark初探——关于分享Spark的一些感悟">
<meta name="twitter:description" content="本文主要用于分享Spark的一些感悟">
<meta name="twitter:image" content="https://crazylook.github.io/images/2014-6-24/01.png">
  
    <link rel="alternate" href="/atom.xml" title="crazylook&#39;s blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">crazylook&#39;s blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Python、Spark、ML、DL、推荐</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://crazylook.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-spark/firstexploration" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/06/24/spark/firstexploration/" class="article-date">
  <time datetime="2014-06-24T04:20:31.000Z" itemprop="datePublished">2014-06-24</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark初探——关于分享Spark的一些感悟
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文主要用于分享Spark的一些感悟<br><a id="more"></a></p>
<blockquote>
<p><strong>关于作者</strong></p>
<ul>
<li>crazylook: pyhton,spark,ml,dl,recsys</li>
<li>blog: <a href="https://crazylook.github.io/">https://crazylook.github.io/</a></li>
<li>email: liuhongbin1990@foxmail.com</li>
</ul>
<p><strong>转载请注明出处：</strong><br><a href="https://crazylook.github.io/2014/06/24/spark-notes-firstexploration/">https://crazylook.github.io/2014/06/24/spark-notes-firstexploration/</a></p>
</blockquote>
<p><strong>前言</strong><br>之前偶尔遇到Spark、Shark这些名词，因为一直对Hadoop研究还有点粗浅，不愿意去接这触个号称更高一级别的Spark。这两天接触了Spark亚太研究院的一个免费公开课，开始揭开Spark的神秘面纱。<br>下面正式开始干货。</p>
<h1 id="1-Spark为什么能够把Hadoop速度提高100倍以上"><a href="#1-Spark为什么能够把Hadoop速度提高100倍以上" class="headerlink" title="1. Spark为什么能够把Hadoop速度提高100倍以上"></a>1. Spark为什么能够把Hadoop速度提高100倍以上</h1><h2 id="1-1-How-fast-to-write-with-scala"><a href="#1-1-How-fast-to-write-with-scala" class="headerlink" title="1.1 How fast to write with scala?"></a>1.1 How fast to write with scala?</h2><h3 id="1-1-1-Spark的WordCount实例"><a href="#1-1-1-Spark的WordCount实例" class="headerlink" title="1.1.1 Spark的WordCount实例"></a>1.1.1 Spark的WordCount实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val file = sc.textFile(&quot;hdfs://...&quot;)</span><br><span class="line">val counts = file.flatMap(line =&gt; line.split(&quot; &quot;)).map(word =&gt; (word, 1)).reduceByKey(_ + _)</span><br><span class="line">counts.saveAsTextFile(“hdfs://…&quot;)</span><br></pre></td></tr></table></figure>
<p>顿时感觉scala编程很有R的味道（简洁、函数式编程）。本人很喜欢，做为本科数学系的人来说就是喜欢函数式编程。</p>
<h3 id="1-1-2-对比一下Wordcount-in-Hadoop"><a href="#1-1-2-对比一下Wordcount-in-Hadoop" class="headerlink" title="1.1.2 对比一下Wordcount in Hadoop"></a>1.1.2 对比一下Wordcount in Hadoop</h3><img src="/images/2014-6-24/01.png" width="400" height="400">
<p>我想我再也不愿意用Java写mapreduce程序了。</p>
<h3 id="1-1-3-对比一下RHadoop写的Wordcount"><a href="#1-1-3-对比一下RHadoop写的Wordcount" class="headerlink" title="1.1.3 对比一下RHadoop写的Wordcount"></a>1.1.3 对比一下RHadoop写的Wordcount</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input&lt;- &apos;/user/hadoop/input&apos;</span><br><span class="line">wordcount.mr&lt;-mapreduce(</span><br><span class="line">    input,</span><br><span class="line">    map=function(k,v)&#123;</span><br><span class="line">        key&lt;-unlist(strsplit(v,&quot; &quot;))</span><br><span class="line">　　    keyval(key,1)</span><br><span class="line">    &#125;</span><br><span class="line">　　,reduce=function(k,v)&#123;</span><br><span class="line">        keyval(k,sum(v))</span><br><span class="line">　　&#125;</span><br><span class="line">)</span><br><span class="line">from.dfs(wordcount.mr)</span><br></pre></td></tr></table></figure>
<p>其实RHadoop编程也很简洁，但是RHadoop还不是一个可用于生产环境的产品。</p>
<h2 id="1-2MapReduce架构"><a href="#1-2MapReduce架构" class="headerlink" title="1.2MapReduce架构"></a>1.2MapReduce架构</h2><img src="/images/2014-6-24/09.png" width="400" height="400">
<h2 id="1-3Why-Hadoop-so-slow"><a href="#1-3Why-Hadoop-so-slow" class="headerlink" title="1.3Why Hadoop so slow?"></a>1.3Why Hadoop so slow?</h2><img src="/images/2014-6-24/06.png" width="400" height="400">
<h2 id="1-4Spark-So-fast"><a href="#1-4Spark-So-fast" class="headerlink" title="1.4Spark? So fast!"></a>1.4Spark? So fast!</h2><img src="/images/2014-6-24/07.png" width="400" height="400">
<p><strong>More reasons!</strong></p>
<ul>
<li>DAG     有向无环图</li>
<li><p>Scheduler   调度</p>
<img src="/images/2014-6-24/08.png" width="400" height="400">
</li>
<li><p>Lineage     血统(容错处理)</p>
</li>
</ul>
<h1 id="2-Spark的内核剖析"><a href="#2-Spark的内核剖析" class="headerlink" title="2. Spark的内核剖析"></a>2. Spark的内核剖析</h1><h2 id="2-1-一些令人激动的图"><a href="#2-1-一些令人激动的图" class="headerlink" title="2.1 一些令人激动的图"></a>2.1 一些令人激动的图</h2><h3 id="2-1-1-上一个高大上的架构图"><a href="#2-1-1-上一个高大上的架构图" class="headerlink" title="2.1.1 上一个高大上的架构图"></a>2.1.1 上一个高大上的架构图</h3><img src="/images/2014-6-24/02.png" width="400" height="400">
<h3 id="2-1-2-One-stack-to-rule-them-all"><a href="#2-1-2-One-stack-to-rule-them-all" class="headerlink" title="2.1.2 One stack to rule them all"></a>2.1.2 One stack to rule them all</h3><p>用Spark，一个团队可以干Hadoop三个不同团队干的活。<br><img src="/images/2014-6-24/03.png" width="400" height="400"></p>
<h3 id="2-1-3-Project-Goals"><a href="#2-1-3-Project-Goals" class="headerlink" title="2.1.3 Project  Goals"></a>2.1.3 Project  Goals</h3><img src="/images/2014-6-24/04.png" width="400" height="400">
<h3 id="2-1-4-Codebase-size"><a href="#2-1-4-Codebase-size" class="headerlink" title="2.1.4 Codebase size"></a>2.1.4 Codebase size</h3><p>这个是一个让我发狂的东东，Spark主要内核代码只有2W行，这点燃了我研究Spark源码的欲望<br><img src="/images/2014-6-24/05.png" width="200" height="200"></p>
<h2 id="2-2-Spark核心概念"><a href="#2-2-Spark核心概念" class="headerlink" title="2.2 Spark核心概念"></a>2.2 Spark核心概念</h2><h3 id="2-2-1-RDD-弹性分布数据集"><a href="#2-2-1-RDD-弹性分布数据集" class="headerlink" title="2.2.1 (RDD)弹性分布数据集"></a>2.2.1 (RDD)弹性分布数据集</h3><ul>
<li>RDD(Resilient Distributed Dataset)是Spark的最基本抽象,是对分布式内存的抽象使用，实现了以操作本地集合的方式来操作分布式数据集的抽象实现。RDD是Spark最核心的东西，它表示已被分区，不可变的并能够被并行操作的数据集合，不同的数据集格式对应不同的RDD实现。RDD必须是可序列化的。RDD可以cache到内存中，每次对RDD数据集的操作之后的结果，都可以存放到内存中，下一个操作可以直接从内存中输入，省去了MapReduce大量的磁盘IO操作。这对于迭代运算比较常见的机器学习算法, 交互式数据挖掘来说，效率提升比较大。</li>
<li>RDD的特点：</li>
</ul>
<ol>
<li>它是在集群节点上的不可变的、已分区的集合对象。</li>
<li>通过并行转换的方式来创建如（map, filter, join, etc）。</li>
<li>失败自动重建。</li>
<li>可以控制存储级别（内存、磁盘等）来进行重用。</li>
<li>必须是可序列化的。</li>
<li>是静态类型的。</li>
</ol>
<ul>
<li>RDD的好处</li>
</ul>
<ol>
<li>RDD只能从持久存储或通过Transformations操作产生，相比于分布式共享内存（DSM）可以更高效实现容错，对于丢失部分数据分区只需根据它的lineage就可重新计算出来，而不需要做特定的Checkpoint。</li>
<li>RDD的不变性，可以实现类Hadoop MapReduce的推测式执行。</li>
<li>RDD的数据分区特性，可以通过数据的本地性来提高性能，这与Hadoop MapReduce是一样的。</li>
<li>RDD都是可序列化的，在内存不足时可自动降级为磁盘存储，把RDD存储于磁盘上，这时性能会有大的下降但不会差于现在的MapReduce。</li>
</ol>
<ul>
<li>RDD的存储与分区</li>
</ul>
<ol>
<li>用户可以选择不同的存储级别存储RDD以便重用。</li>
<li>当前RDD默认是存储于内存，但当内存不足时，RDD会spill到disk。</li>
<li>RDD在需要进行分区把数据分布于集群中时会根据每条记录Key进行分区（如Hash 分区），以此保证两个数据集在Join时能高效。</li>
</ol>
<ul>
<li>RDD的内部表示<br>在RDD的内部实现中每个RDD都可以使用5个方面的特性来表示：</li>
</ul>
<ol>
<li>分区列表（数据块列表）</li>
<li>计算每个分片的函数（根据父RDD计算出此RDD）</li>
<li>对父RDD的依赖列表</li>
<li>对key-value RDD的Partitioner【可选】</li>
<li>每个数据分片的预定义地址列表(如HDFS上的数据块的地址)【可选】</li>
</ol>
<ul>
<li><p>RDD的存储级别<br>RDD根据useDisk、useMemory、deserialized、replication四个参数的组合提供了11种存储级别：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val DISK_ONLY = new StorageLevel(true, false, false)</span><br><span class="line">val DISK_ONLY_2 = new StorageLevel(true, false, false, 2)</span><br><span class="line">val MEMORY_ONLY = new StorageLevel(false, true, true)</span><br><span class="line">val MEMORY_ONLY_2 = new StorageLevel(false, true, true, 2)</span><br><span class="line">val MEMORY_ONLY_SER = new StorageLevel(false, true, false)</span><br><span class="line">val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, 2)</span><br><span class="line">val MEMORY_AND_DISK = new StorageLevel(true, true, true)</span><br><span class="line">val MEMORY_AND_DISK_2 = new StorageLevel(true, true, true, 2)</span><br><span class="line">val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false)</span><br><span class="line">val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, 2)</span><br></pre></td></tr></table></figure>
</li>
<li><p>RDD定义了各种操作，不同类型的数据由不同的RDD类抽象表示，不同的操作也由RDD进行抽实现。</p>
</li>
</ul>
<h3 id="2-2-2-Lineage（血统）"><a href="#2-2-2-Lineage（血统）" class="headerlink" title="2.2.2 Lineage（血统）"></a>2.2.2 Lineage（血统）</h3><ul>
<li>利用内存加快数据加载,在众多的其它的In-Memory类数据库或Cache类系统中也有实现，Spark的主要区别在于它处理分布式运算环境下的数据容错性（节点实效/数据丢失）问题时采用的方案。为了保证RDD中数据的鲁棒性，RDD数据集通过所谓的血统关系(Lineage)记住了它是如何从其它RDD中演变过来的。相比其它系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的特定数据转换（Transformation）操作（filter, map, join etc.)行为。当这个RDD的部分分区数据丢失时，它可以通过Lineage获取足够的信息来重新运算和恢复丢失的数据分区。这种粗颗粒的数据模型，限制了Spark的运用场合，但同时相比细颗粒度的数据模型，也带来了性能的提升。</li>
<li>RDD在Lineage依赖方面分为两种Narrow Dependencies与Wide Dependencies用来解决数据容错的高效性。Narrow Dependencies是指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区或多个父RDD的分区对应于一个子RDD的分区，也就是说一个父RDD的一个分区不可能对应一个子RDD的多个分区。Wide Dependencies是指子RDD的分区依赖于父RDD的多个分区或所有分区，也就是说存在一个父RDD的一个分区对应一个子RDD的多个分区。对与Wide Dependencies，这种计算的输入和输出在不同的节点上，lineage方法对与输入节点完好，而输出节点宕机时，通过重新计算，这种情况下，这种方法容错是有效的，否则无效，因为无法重试，需要向上其祖先追溯看是否可以重试（这就是lineage，血统的意思），Narrow Dependencies对于数据的重算开销要远小于Wide Dependencies的数据重算开销。</li>
</ul>
<h3 id="2-2-3-容错"><a href="#2-2-3-容错" class="headerlink" title="2.2.3 容错"></a>2.2.3 容错</h3><ul>
<li>在RDD计算，通过checkpint进行容错，做checkpoint有两种方式，一个是checkpoint data，一个是logging the updates。用户可以控制采用哪种方式来实现容错，默认是logging the updates方式，通过记录跟踪所有生成RDD的转换（transformations）也就是记录每个RDD的lineage（血统）来重新计算生成丢失的分区数据。</li>
</ul>
<h3 id="2-2-4-资源管理与作业调度"><a href="#2-2-4-资源管理与作业调度" class="headerlink" title="2.2.4 资源管理与作业调度"></a>2.2.4 资源管理与作业调度</h3><ul>
<li>Spark对于资源管理与作业调度可以使用Standalone(独立模式)，Apache Mesos及Hadoop YARN来实现。 Spark on Yarn在Spark0.6时引用，但真正可用是在现在的branch-0.8版本。Spark on Yarn遵循YARN的官方规范实现，得益于Spark天生支持多种Scheduler和Executor的良好设计，对YARN的支持也就非常容易，Spark on Yarn的大致框架图。<img src="/images/2014-6-24/10.png" width="600" height="600"></li>
<li>让Spark运行于YARN上与Hadoop共用集群资源可以提高资源利用率。</li>
</ul>
<h3 id="2-2-5-Scala"><a href="#2-2-5-Scala" class="headerlink" title="2.2.5 Scala"></a>2.2.5 Scala</h3><ul>
<li>Spark使用Scala开发，默认使用Scala作为编程语言。编写Spark程序比编写Hadoop MapReduce程序要简单的多，SparK提供了Spark-Shell，可以在Spark-Shell测试程序。写SparK程序的一般步骤就是创建或使用(SparkContext)实例，使用SparkContext创建RDD，然后就是对RDD进行操作。如：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val sc = new SparkContext(master, appName, [sparkHome], [jars])</span><br><span class="line">val textFile = sc.textFile(&quot;hdfs://.....&quot;)</span><br><span class="line">textFile.map(....).filter(.....).....</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>Scala是我接触的除了R又一个令我激动的语言，有机会要好好研究一下。</p>
<h1 id="3-Spark集群案例解析，包含集群搭建"><a href="#3-Spark集群案例解析，包含集群搭建" class="headerlink" title="3.Spark集群案例解析，包含集群搭建"></a>3.Spark集群案例解析，包含集群搭建</h1><h2 id="3-1-Standalone模式"><a href="#3-1-Standalone模式" class="headerlink" title="3.1 Standalone模式"></a>3.1 Standalone模式</h2><ul>
<li>为方便Spark的推广使用，Spark提供了Standalone模式，Spark一开始就设计运行于Apache Mesos资源管理框架上，这是非常好的设计，但是却带了部署测试的复杂性。为了让Spark能更方便的部署和尝试，Spark因此提供了Standalone运行模式，它由一个Spark Master和多个Spark worker组成，与Hadoop MapReduce1很相似，就连集群启动方式都几乎是一样。</li>
<li>以Standalone模式运行Spark集群</li>
</ul>
<ol>
<li>下载Scala2.9.3，并配置SCALA_HOME</li>
<li>下载Spark代码（可以使用源码编译也可以下载编译好的版本）这里下载 编译好的版本（<a href="http://spark-project.org/download/spark-0.7.3-prebuilt-cdh4.tgz）" target="_blank" rel="noopener">http://spark-project.org/download/spark-0.7.3-prebuilt-cdh4.tgz）</a></li>
<li>解压spark-0.7.3-prebuilt-cdh4.tgz安装包</li>
<li><p>修改配置（conf/*） slaves: 配置工作节点的主机名 spark-env.sh：配置环境变量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SCALA_HOME=/home/spark/scala-2.9.3</span><br><span class="line">JAVA_HOME=/home/spark/jdk1.6.0_45</span><br><span class="line">SPARK_MASTER_IP=spark1             </span><br><span class="line">SPARK_MASTER_PORT=30111</span><br><span class="line">SPARK_MASTER_WEBUI_PORT=30118</span><br><span class="line">SPARK_WORKER_CORES=2 SPARK_WORKER_MEMORY=4g</span><br><span class="line">SPARK_WORKER_PORT=30333</span><br><span class="line">SPARK_WORKER_WEBUI_PORT=30119</span><br><span class="line">SPARK_WORKER_INSTANCES=1</span><br></pre></td></tr></table></figure>
</li>
<li><p>把Hadoop配置copy到conf目录下</p>
</li>
<li>在master主机上对其它机器做ssh无密码登录</li>
<li>把配置好的Spark程序使用scp copy到其它机器</li>
<li>在master启动集群<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$SPARK_HOME/start-all.sh</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="3-2-yarn模式"><a href="#3-2-yarn模式" class="headerlink" title="3.2 yarn模式"></a>3.2 yarn模式</h2><ul>
<li>Spark-shell现在还不支持Yarn模式，使用Yarn模式运行，需要把Spark程序全部打包成一个jar包提交到Yarn上运行。目录只有branch-0.8版本才真正支持Yarn。</li>
<li>以Yarn模式运行Spark</li>
</ul>
<ol>
<li>下载Spark代码.<br><code>git clone git://github.com/mesos/spark</code></li>
<li><p>切换到branch-0.8</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd spark</span><br><span class="line">git checkout -b yarn --track origin/yarn</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用sbt编译Spark并</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$SPARK_HOME/sbt/sbt</span><br><span class="line">package</span><br><span class="line">assembly</span><br></pre></td></tr></table></figure>
</li>
<li><p>把Hadoop yarn配置copy到conf目录下</p>
</li>
<li>运行测试<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> SPARK_JAR=./core/target/scala-2.9.3/spark-core-assembly-0.8.0-SNAPSHOT.jar \</span><br><span class="line">./run spark.deploy.yarn.Client --jar examples/target/scala-2.9.3/ \</span><br><span class="line">--class spark.examples.SparkPi --args yarn-standalone</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="3-3-使用Spark-shell"><a href="#3-3-使用Spark-shell" class="headerlink" title="3.3 使用Spark-shell"></a>3.3 使用Spark-shell</h2><ul>
<li>Spark-shell使用很简单，当Spark以Standalon模式运行后，使用$SPARK_HOME/spark-shell进入shell即可，在Spark-shell中SparkContext已经创建好了，实例名为sc可以直接使用，还有一个需要注意的是，在Standalone模式下，Spark默认使用的调度器的FIFO调度器而不是公平调度，而Spark-shell作为一个Spark程序一直运行在Spark上，其它的Spark程序就只能排队等待，也就是说同一时间只能有一个Spark-shell在运行。</li>
<li>在Spark-shell上写程序非常简单，就像在Scala Shell上写程序一样。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val textFile = sc.textFile(&quot;hdfs://hadoop1:2323/user/data&quot;)</span><br><span class="line">textFile: spark.RDD[String] = spark.MappedRDD@2ee9b6e3</span><br><span class="line"></span><br><span class="line">scala&gt; textFile.count() // Number of items in this RDD</span><br><span class="line">res0: Long = 21374</span><br><span class="line"></span><br><span class="line">scala&gt; textFile.first() // First item in this RDD</span><br><span class="line">res1: String = # Spark</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="3-4-编写Driver程序"><a href="#3-4-编写Driver程序" class="headerlink" title="3.4 编写Driver程序"></a>3.4 编写Driver程序</h2><ul>
<li>在Spark中Spark程序称为Driver程序，编写Driver程序很简单几乎与在Spark-shell上写程序是一样的，不同的地方就是SparkContext需要自己创建。如WorkCount程序如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line"></span><br><span class="line">object WordCount &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    if (args.length ==0 )&#123;</span><br><span class="line">      println(&quot;usage is org.test.WordCount &lt;master&gt;&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    println(&quot;the args: &quot;)</span><br><span class="line">    args.foreach(println)</span><br><span class="line"></span><br><span class="line">    val hdfsPath = &quot;hdfs://hadoop1:8020&quot;</span><br><span class="line"></span><br><span class="line">    // create the SparkContext， args(0)由yarn传入appMaster地址</span><br><span class="line">    val sc = new SparkContext(args(0), &quot;WrodCount&quot;,</span><br><span class="line">    System.getenv(&quot;SPARK_HOME&quot;), Seq(System.getenv(&quot;SPARK_TEST_JAR&quot;)))</span><br><span class="line"></span><br><span class="line">    val textFile = sc.textFile(hdfsPath + args(1))</span><br><span class="line"></span><br><span class="line">    val result = textFile.flatMap(line =&gt; line.split(&quot;\\s+&quot;))</span><br><span class="line">        .map(word =&gt; (word, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    result.saveAsTextFile(hdfsPath + args(2))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>附：一篇介绍Spark比较到位的Page</strong><br><a href="http://tech.uc.cn/?p=2116" target="_blank" rel="noopener">http://tech.uc.cn/?p=2116</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://crazylook.github.io/2014/06/24/spark/firstexploration/" data-id="cjdfhmrk8000xqt66hl1ifaji" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2014/06/30/demo/vb-project-backup/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          研二这一年那点项目
        
      </div>
    </a>
  
  
    <a href="/2014/06/18/demo/mahout-notes-day1/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">Mahout开发环境搭建及推荐系统初探</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Kategorien</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hive/">Hive</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/">Spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/recsys/">recsys</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/那些年的小项目/">那些年的小项目</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ConfigParser/">ConfigParser</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo优化/">Hexo优化</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HiveUDF/">HiveUDF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive操作/">Hive操作</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive查询/">Hive查询</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ItemCF/">ItemCF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mahout/">Mahout</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RHadoop/">RHadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RHive/">RHive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/">git</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/正则表达式/">正则表达式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/研二/">研二</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/项目/">项目</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ConfigParser/" style="font-size: 10px;">ConfigParser</a> <a href="/tags/Hexo优化/" style="font-size: 10px;">Hexo优化</a> <a href="/tags/HiveUDF/" style="font-size: 10px;">HiveUDF</a> <a href="/tags/Hive操作/" style="font-size: 10px;">Hive操作</a> <a href="/tags/Hive查询/" style="font-size: 10px;">Hive查询</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Mahout/" style="font-size: 10px;">Mahout</a> <a href="/tags/RHadoop/" style="font-size: 10px;">RHadoop</a> <a href="/tags/RHive/" style="font-size: 10px;">RHive</a> <a href="/tags/Spark/" style="font-size: 10px;">Spark</a> <a href="/tags/git/" style="font-size: 10px;">git</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/正则表达式/" style="font-size: 10px;">正则表达式</a> <a href="/tags/研二/" style="font-size: 10px;">研二</a> <a href="/tags/项目/" style="font-size: 10px;">项目</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">February 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/09/">September 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/06/">June 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/05/">May 2014</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/01/18/recsys/spark-CF/">Spark中实现ItemCF、UserCF</a>
          </li>
        
          <li>
            <a href="/2015/02/06/python/notes-Regulator/">Python 正则表达式</a>
          </li>
        
          <li>
            <a href="/2015/02/06/python/notes-ConfigParser/">Python ConfigParser的使用</a>
          </li>
        
          <li>
            <a href="/2014/09/09/hexo/git-notes1/">git使用记录</a>
          </li>
        
          <li>
            <a href="/2014/06/30/demo/vb-project-backup/">研二这一年那点项目</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 crazylook<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>